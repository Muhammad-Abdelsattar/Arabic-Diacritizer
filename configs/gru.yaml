data:
  train_files: ["data/processed/all-data.txt"]
  val_files: null
  test_files: null
  batch_size: 1024
  num_workers: 12
  cache_dir: ".cache/"
  cache_format: "npz"
  max_length: 512
  val_split: 0.01
  test_split: 0
  diacritic_keep_probs: [0.0, 0.0, 0.0]

modeling_config:
  architecture:
    name: "bigru" 
    
    # Hyperparameters for the BiGRUDiacritizer class
    embedding_dim: 96
    hidden_dim: 96
    num_layers: 2
    dropout: 0.3
  
  optimizer:
    name: "adamw"
    lr: 0.0008
    weight_decay: 0.01

  scheduler: 
    name: "cosine"
    t_max: 50

trainer:
  max_epochs: 50
